{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZPijSwUfTOHalSSRutFwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekuelkpodar/AIAcademy/blob/main/Building_transformers_for_LLMs_with_Google_Colab_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "S8-6cNllFQwa",
        "outputId": "2f2672aa-d27c-4e7c-8814-2709fc8bc69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Visualizing attention for \"hello world\":\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print('\\n' + '='*50)\n",
        "print('Visualizing attention for \"hello world\":')\n",
        "print('='*50)\n",
        "\n",
        "visualize_attention(model, \"hello world\", \"bonjour monde\", src_vocab, tgt_vocab, device, layer_idx=0, head_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "f24b4589",
        "outputId": "00c0d802-f23a-4a94-d2d4-6e11d5214b46"
      },
      "source": [
        "print('\\n' + '='*50)\n",
        "print('Visualizing attention for \"hello world\":')\n",
        "print('='*50)\n",
        "\n",
        "visualize_attention(model, \"hello world\", \"bonjour monde\", src_vocab, tgt_vocab, device, layer_idx=0, head_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Visualizing attention for \"hello world\":\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "553d7948",
        "outputId": "4529914c-462e-4f0c-d6b6-fe2509e22ac8"
      },
      "source": [
        "print('\\n' + '='*50)\n",
        "print('Visualizing attention for \"hello world\":')\n",
        "print('='*50)\n",
        "\n",
        "visualize_attention(model, \"hello world\", \"bonjour monde\", src_vocab, tgt_vocab, device, layer_idx=0, head_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Visualizing attention for \"hello world\":\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515e4748"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Optimization Techniques\n",
        "\n",
        "### 9.1 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c84cca06"
      },
      "source": [
        "class WarmupScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        lr = self.d_model ** (-0.5) * min(self.step_num ** (-0.5),\n",
        "                                          self.step_num * self.warmup_steps ** (-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11c18a22"
      },
      "source": [
        "### 9.2 Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5cdc256"
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, vocab_size=None, padding_idx=0):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.vocab_size = vocab_size\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: Predictions [batch_size * seq_len, vocab_size]\n",
        "            target: Ground truth labels [batch_size * seq_len]\n",
        "        \"\"\"\n",
        "        vocab_size = pred.size(-1)\n",
        "\n",
        "        # Create smoothed target distribution\n",
        "        smooth_target = torch.zeros_like(pred)\n",
        "        smooth_target.fill_(self.smoothing / (vocab_size - 2))  # -2 for true class and padding\n",
        "        smooth_target.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        smooth_target[:, self.padding_idx] = 0\n",
        "\n",
        "        # Mask padding positions\n",
        "        mask = (target != self.padding_idx).unsqueeze(1)\n",
        "        smooth_target = smooth_target * mask\n",
        "\n",
        "        # Calculate loss\n",
        "        log_probs = F.log_softmax(pred, dim=-1)\n",
        "        loss = -(smooth_target * log_probs).sum(dim=-1)\n",
        "        loss = loss[mask.squeeze()].mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952df601"
      },
      "source": [
        "### 9.3 Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09752838"
      },
      "source": [
        "def train_with_accumulation(model, train_loader, criterion, optimizer, device,\n",
        "                           accumulation_steps=4, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Training with gradient accumulation\n",
        "\n",
        "    Args:\n",
        "        accumulation_steps: Number of steps to accumulate gradients\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            # Calculate loss and normalize by accumulation steps\n",
        "            loss = criterion(output, tgt_output) / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights every accumulation_steps\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed. Average Loss: {total_loss/len(train_loader):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7955c89b"
      },
      "source": [
        "### 9.4 Mixed Precision Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "958de3ac"
      },
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def train_mixed_precision(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Training with automatic mixed precision (FP16)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with autocast\n",
        "            with autocast():\n",
        "                output = model(src, tgt_input)\n",
        "                output = output.reshape(-1, output.shape[-1])\n",
        "                tgt_output = tgt_output.reshape(-1)\n",
        "                loss = criterion(output, tgt_output)\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed. Average Loss: {total_loss/len(train_loader):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6ff210"
      },
      "source": [
        "### 9.5 Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfc098f9"
      },
      "source": [
        "def visualize_attention(model, src_sentence, tgt_sentence, src_vocab, tgt_vocab,\n",
        "                       device, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    Visualize attention weights for a specific layer and head\n",
        "\n",
        "    Args:\n",
        "        layer_idx: Which encoder/decoder layer to visualize\n",
        "        head_idx: Which attention head to visualize\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode sentences\n",
        "    src_tokens = ['<sos>'] + src_sentence.lower().split() + ['<eos>']\n",
        "    tgt_tokens = ['<sos>'] + tgt_sentence.lower().split() + ['<eos>']\n",
        "\n",
        "    src_indices = torch.tensor([[src_vocab.get(t, src_vocab['<unk>']) for t in src_tokens]]).to(device)\n",
        "    tgt_indices = torch.tensor([[tgt_vocab.get(t, tgt_vocab['<unk>']) for t in tgt_tokens]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_mask = model.make_src_mask(src_indices)\n",
        "        encoder_output, encoder_self_attention_weights = model.encode(src_indices, src_mask)\n",
        "\n",
        "        # To visualize cross-attention, we need decoder output too.\n",
        "        # The decode function expects a target input. We'll use the full target sentence for visualization.\n",
        "        tgt_mask = model.make_tgt_mask(tgt_indices)\n",
        "        _, decoder_self_attention_weights, decoder_cross_attention_weights = \\\n",
        "            model.decode(tgt_indices, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Determine which attention weights to visualize based on layer_idx.\n",
        "        # For simplicity, we'll focus on visualizing decoder cross-attention weights, as they show source-target alignment.\n",
        "        # The weights are of shape [batch_size, num_heads, tgt_seq_len, src_seq_len]\n",
        "        if layer_idx < len(decoder_cross_attention_weights):\n",
        "            attention_weights = decoder_cross_attention_weights[layer_idx] # Shape: [1, num_heads, tgt_len, src_len]\n",
        "            if head_idx < attention_weights.shape[1]:\n",
        "                attention_weights = attention_weights.squeeze(0)[head_idx].cpu().numpy() # Shape: [tgt_len, src_len]\n",
        "                title = f'Decoder Cross-Attention Weights - Layer {layer_idx}, Head {head_idx}'\n",
        "            else:\n",
        "                print(f\"Warning: head_idx {head_idx} out of range for layer {layer_idx}. Defaulting to head 0.\")\n",
        "                attention_weights = attention_weights.squeeze(0)[0].cpu().numpy()\n",
        "                title = f'Decoder Cross-Attention Weights - Layer {layer_idx}, Head 0 (Default)'\n",
        "        else:\n",
        "            print(f\"Warning: layer_idx {layer_idx} out of range for decoder cross-attention layers. Defaulting to first layer/head if available.\")\n",
        "            if decoder_cross_attention_weights:\n",
        "                attention_weights = decoder_cross_attention_weights[0].squeeze(0)[0].cpu().numpy()\n",
        "                title = 'Decoder Cross-Attention Weights - Layer 0, Head 0 (Default)'\n",
        "            else:\n",
        "                print(\"No decoder cross-attention weights available.\")\n",
        "                plt.text(0.5, 0.5, \"No Attention Weights to Display\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
        "                plt.title('Attention Visualization (No Weights)')\n",
        "                plt.show()\n",
        "                return\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(attention_weights, cmap='viridis')\n",
        "    plt.xlabel('Source Tokens')\n",
        "    plt.ylabel('Target Tokens')\n",
        "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45)\n",
        "    plt.yticks(range(len(tgt_tokens)), tgt_tokens)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef5f3e0"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Advanced Techniques for Scaling to LLMs\n",
        "\n",
        "### 10.1 Decoder-Only Architecture (GPT-style)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7757099c"
      },
      "source": [
        "class GPTTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder-only transformer (GPT-style) for language modeling\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12,\n",
        "                 d_ff=3072, max_seq_len=1024, dropout=0.1):\n",
        "        super(GPTTransformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Decoder layers only (no encoder)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "        mask = mask.to(x.device)\n",
        "\n",
        "        # Embed and add positional encoding\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Single decoder block for GPT-style models\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention with causal mask\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask.unsqueeze(0).unsqueeze(0))\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d82d6903"
      },
      "source": [
        "### 10.2 Rotary Position Embeddings (RoPE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ea94d28"
      },
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary Position Embedding (RoPE) as used in models like LLaMA\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, num_heads, head_dim]\n",
        "            seq_len: Sequence length\n",
        "        \"\"\"\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        return emb.cos()[None, :, None, :], emb.sin()[None, :, None, :]\n",
        "\n",
        "def apply_rotary_emb(x, cos, sin):\n",
        "    \"\"\"Apply rotary embeddings to input tensor\"\"\"\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat([\n",
        "        x1 * cos - x2 * sin,\n",
        "        x1 * sin + x2 * cos\n",
        "    ], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14b31954"
      },
      "source": [
        "### 10.3 Key-Value Caching for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c22ef2b5"
      },
      "source": [
        "def generate_with_cache(model, prompt_ids, max_new_tokens=50, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text with KV caching for efficient inference\n",
        "\n",
        "    Args:\n",
        "        model: GPT-style transformer model\n",
        "        prompt_ids: Input token IDs [1, prompt_len]\n",
        "        max_new_tokens: Number of tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_k: Top-k sampling parameter\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = prompt_ids\n",
        "    past_key_values = None\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            # Forward pass (with caching in production models)\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9d5124e"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Best Practices and Tips\n",
        "\n",
        "### Memory Optimization\n",
        "1. **Gradient Checkpointing**: Trade compute for memory\n",
        "2. **Mixed Precision**: Use FP16 for faster training\n",
        "3. **Batch Size Tuning**: Find optimal batch size for your GPU\n",
        "4. **Gradient Accumulation**: Simulate larger batches\n",
        "\n",
        "### Training Stability\n",
        "1. **Layer Normalization**: Use pre-norm (before attention) for deeper models\n",
        "2. **Gradient Clipping**: Prevent exploding gradients\n",
        "3. **Warmup**: Gradually increase learning rate\n",
        "4. **Weight Initialization**: Use Xavier or He initialization\n",
        "\n",
        "### Hyperparameter Recommendations\n",
        "```python\n",
        "# Small model (testing)\n",
        "d_model = 256\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "d_ff = 1024\n",
        "\n",
        "# Medium model (GPT-2 small)\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "d_ff = 3072\n",
        "\n",
        "# Large model (GPT-2 large)\n",
        "d_model = 1280\n",
        "num_heads = 20\n",
        "num_layers = 36\n",
        "d_ff = 5120\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Common Issues and Solutions\n",
        "\n",
        "### Issue 1: Out of Memory\n",
        "**Solutions:**\n",
        "- Reduce batch size\n",
        "- Use gradient accumulation\n",
        "- Enable mixed precision training\n",
        "- Use gradient checkpointing\n",
        "\n",
        "### Issue 2: Loss Not Decreasing\n",
        "**Solutions:**\n",
        "- Check learning rate (try 1e-4 to 1e-3)\n",
        "- Verify data preprocessing\n",
        "- Check for gradient clipping\n",
        "- Ensure masks are correct\n",
        "\n",
        "### Issue 3: Poor Generation Quality\n",
        "**Solutions:**\n",
        "- Train longer\n",
        "- Increase model size\n",
        "- Use better tokenization\n",
        "- Implement beam search instead of greedy decoding\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Further Reading and Resources\n",
        "\n",
        "### Papers\n",
        "1. **\"Attention Is All You Need\"** (Vaswani et al., 2017) - Original transformer paper\n",
        "2. **\"BERT\"** (Devlin et al., 2018) - Bidirectional transformers\n",
        "3. **\"GPT-2\"** (Radford et al., 2019) - Language models are unsupervised multitask learners\n",
        "4. **\"GPT-3\"** (Brown et al., 2020) - Few-shot learning with large models\n",
        "\n",
        "### Online Resources\n",
        "- Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/\n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers/\n",
        "- PyTorch Tutorials: https://pytorch.org/tutorials/\n",
        "\n",
        "### Next Steps\n",
        "1. Implement attention visualization\n",
        "2. Add beam search decoding\n",
        "3. Experiment with different positional encodings\n",
        "4. Train on larger datasets (WMT, WikiText)\n",
        "5. Implement model parallelism for very large models\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This guide provides a complete foundation for understanding and implementing transformers. The architecture you've built here is the same fundamental design powering modern LLMs, just at different scales. To scale to production LLMs:\n",
        "\n",
        "1. **Scale up**: More layers, larger dimensions, more data\n",
        "2. **Optimize**: Use efficient attention variants (FlashAttention)\n",
        "3. **Distribute**: Model and data parallelism across GPUs\n",
        "4. **Fine-tune**: Instruction tuning, RLHF for alignment\n",
        "\n",
        "Happy building! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34137234"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Advanced Techniques for Scaling to LLMs\n",
        "\n",
        "### 10.1 Decoder-Only Architecture (GPT-style)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f429ef91"
      },
      "source": [
        "class GPTTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder-only transformer (GPT-style) for language modeling\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12,\n",
        "                 d_ff=3072, max_seq_len=1024, dropout=0.1):\n",
        "        super(GPTTransformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Decoder layers only (no encoder)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "        mask = mask.to(x.device)\n",
        "\n",
        "        # Embed and add positional encoding\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Single decoder block for GPT-style models\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention with causal mask\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask.unsqueeze(0).unsqueeze(0))\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6482fa9d"
      },
      "source": [
        "### 10.2 Rotary Position Embeddings (RoPE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0722e80a"
      },
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary Position Embedding (RoPE) as used in models like LLaMA\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, num_heads, head_dim]\n",
        "            seq_len: Sequence length\n",
        "        \"\"\"\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        return emb.cos()[None, :, None, :], emb.sin()[None, :, None, :]\n",
        "\n",
        "def apply_rotary_emb(x, cos, sin):\n",
        "    \"\"\"Apply rotary embeddings to input tensor\"\"\"\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat([\n",
        "        x1 * cos - x2 * sin,\n",
        "        x1 * sin + x2 * cos\n",
        "    ], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e525f041"
      },
      "source": [
        "### 10.3 Key-Value Caching for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d791a787"
      },
      "source": [
        "def generate_with_cache(model, prompt_ids, max_new_tokens=50, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text with KV caching for efficient inference\n",
        "\n",
        "    Args:\n",
        "        model: GPT-style transformer model\n",
        "        prompt_ids: Input token IDs [1, prompt_len]\n",
        "        max_new_tokens: Number of tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_k: Top-k sampling parameter\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = prompt_ids\n",
        "    past_key_values = None\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            # Forward pass (with caching in production models)\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556898fc"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Best Practices and Tips\n",
        "\n",
        "### Memory Optimization\n",
        "1. **Gradient Checkpointing**: Trade compute for memory\n",
        "2. **Mixed Precision**: Use FP16 for faster training\n",
        "3. **Batch Size Tuning**: Find optimal batch size for your GPU\n",
        "4. **Gradient Accumulation**: Simulate larger batches\n",
        "\n",
        "### Training Stability\n",
        "1. **Layer Normalization**: Use pre-norm (before attention) for deeper models\n",
        "2. **Gradient Clipping**: Prevent exploding gradients\n",
        "3. **Warmup**: Gradually increase learning rate\n",
        "4. **Weight Initialization**: Use Xavier or He initialization\n",
        "\n",
        "### Hyperparameter Recommendations\n",
        "```python\n",
        "# Small model (testing)\n",
        "d_model = 256\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "d_ff = 1024\n",
        "\n",
        "# Medium model (GPT-2 small)\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "d_ff = 3072\n",
        "\n",
        "# Large model (GPT-2 large)\n",
        "d_model = 1280\n",
        "num_heads = 20\n",
        "num_layers = 36\n",
        "d_ff = 5120\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Common Issues and Solutions\n",
        "\n",
        "### Issue 1: Out of Memory\n",
        "**Solutions:**\n",
        "- Reduce batch size\n",
        "- Use gradient accumulation\n",
        "- Enable mixed precision training\n",
        "- Use gradient checkpointing\n",
        "\n",
        "### Issue 2: Loss Not Decreasing\n",
        "**Solutions:**\n",
        "- Check learning rate (try 1e-4 to 1e-3)\n",
        "- Verify data preprocessing\n",
        "- Check for gradient clipping\n",
        "- Ensure masks are correct\n",
        "\n",
        "### Issue 3: Poor Generation Quality\n",
        "**Solutions:**\n",
        "- Train longer\n",
        "- Increase model size\n",
        "- Use better tokenization\n",
        "- Implement beam search instead of greedy decoding\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Further Reading and Resources\n",
        "\n",
        "### Papers\n",
        "1. **\"Attention Is All You Need\"** (Vaswani et al., 2017) - Original transformer paper\n",
        "2. **\"BERT\"** (Devlin et al., 2018) - Bidirectional transformers\n",
        "3. **\"GPT-2\"** (Radford et al., 2019) - Language models are unsupervised multitask learners\n",
        "4. **\"GPT-3\"** (Brown et al., 2020) - Few-shot learning with large models\n",
        "\n",
        "### Online Resources\n",
        "- Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/\n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers/\n",
        "- PyTorch Tutorials: https://pytorch.org/tutorials/\n",
        "\n",
        "### Next Steps\n",
        "1. Implement attention visualization\n",
        "2. Add beam search decoding\n",
        "3. Experiment with different positional encodings\n",
        "4. Train on larger datasets (WMT, WikiText)\n",
        "5. Implement model parallelism for very large models\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This guide provides a complete foundation for understanding and implementing transformers. The architecture you've built here is the same fundamental design powering modern LLMs, just at different scales. To scale to production LLMs:\n",
        "\n",
        "1. **Scale up**: More layers, larger dimensions, more data\n",
        "2. **Optimize**: Use efficient attention variants (FlashAttention)\n",
        "3. **Distribute**: Model and data parallelism across GPUs\n",
        "4. **Fine-tune**: Instruction tuning, RLHF for alignment\n",
        "\n",
        "Happy building! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92e0b3ed"
      },
      "source": [
        "# Building Transformers That Power Large Language Models: A Comprehensive Guide\n",
        "\n",
        "## Table of Contents\n",
        "1. Introduction to Transformers\n",
        "2. Core Architecture Components\n",
        "3. Mathematical Foundations\n",
        "4. Implementation Strategy\n",
        "5. Google Colab Setup\n",
        "6. Complete Code Implementation\n",
        "7. Training and Evaluation\n",
        "8. Optimization Techniques\n",
        "9. Advanced Techniques for Scaling to LLMs\n",
        "10. Best Practices and Tips\n",
        "11. Common Issues and Solutions\n",
        "12. Further Reading and Resources\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Introduction to Transformers\n",
        "\n",
        "Transformers are the foundational architecture behind modern large language models (LLMs) like GPT, BERT, and Claude. Introduced in the 2017 paper \"Attention Is All You Need\" by Vaswani et al., transformers revolutionized natural language processing by replacing recurrent architectures with attention mechanisms.\n",
        "\n",
        "### Key Innovation\n",
        "The transformer's core innovation is the **self-attention mechanism**, which allows the model to weigh the importance of different words in a sequence when processing each word, enabling parallel processing and capturing long-range dependencies.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Core Architecture Components\n",
        "\n",
        "### 2.1 Multi-Head Self-Attention\n",
        "The attention mechanism computes relationships between all positions in a sequence simultaneously.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "```\n",
        "Attention(Q, K, V) = softmax(QK^T / d_k)V\n",
        "```\n",
        "\n",
        "Where:\n",
        "- Q (Query): What we're looking for\n",
        "- K (Key): What we have\n",
        "- V (Value): The actual content\n",
        "- d_k: Dimension of key vectors (for scaling)\n",
        "\n",
        "**Multi-head attention** runs multiple attention operations in parallel, allowing the model to attend to different aspects of the input.\n",
        "\n",
        "### 2.2 Position-wise Feed-Forward Networks\n",
        "After attention, each position passes through a two-layer fully-connected network independently:\n",
        "\n",
        "```\n",
        "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "```\n",
        "\n",
        "This adds non-linearity and allows the model to process attended information.\n",
        "\n",
        "### 2.3 Positional Encoding\n",
        "Since transformers process all tokens in parallel, we need to inject position information:\n",
        "\n",
        "```\n",
        "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "```\n",
        "\n",
        "### 2.4 Layer Normalization and Residual Connections\n",
        "Each sub-layer uses residual connections followed by layer normalization:\n",
        "\n",
        "```\n",
        "LayerNorm(x + Sublayer(x))\n",
        "```\n",
        "\n",
        "This stabilizes training and enables deeper networks.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Mathematical Foundations\n",
        "\n",
        "### 3.1 Scaled Dot-Product Attention\n",
        "\n",
        "The attention score between query q and key k is computed as:\n",
        "\n",
        "```\n",
        "score = (q  k) / d_k\n",
        "```\n",
        "\n",
        "Scaling by d_k prevents gradients from becoming too small when d_k is large.\n",
        "\n",
        "### 3.2 Softmax for Attention Weights\n",
        "\n",
        "```\n",
        "attention_weights = softmax(scores) = exp(scores_i) /  exp(scores_j)\n",
        "```\n",
        "\n",
        "This converts scores into a probability distribution.\n",
        "\n",
        "### 3.3 Multi-Head Mechanism\n",
        "\n",
        "Instead of one attention function with d_model dimensions, we use h parallel attention heads:\n",
        "\n",
        "```\n",
        "MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n",
        "\n",
        "where head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Implementation Strategy\n",
        "\n",
        "### Building Blocks Hierarchy:\n",
        "1. **Positional Encoding**: Add position information\n",
        "2. **Scaled Dot-Product Attention**: Core attention mechanism\n",
        "3. **Multi-Head Attention**: Multiple parallel attention operations\n",
        "4. **Feed-Forward Network**: Position-wise processing\n",
        "5. **Encoder Layer**: Attention + FFN with residual connections\n",
        "6. **Decoder Layer**: Masked attention + cross-attention + FFN\n",
        "7. **Full Transformer**: Stack of encoder and decoder layers\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Google Colab Setup\n",
        "\n",
        "### Step 1: Open Google Colab\n",
        "Navigate to [colab.research.google.com](https://colab.research.google.com)\n",
        "\n",
        "### Step 2: Enable GPU (Recommended)\n",
        "- Runtime  Change runtime type  Hardware accelerator  GPU  Save\n",
        "\n",
        "### Step 3: Install Required Libraries\n",
        "Most libraries are pre-installed, but ensure you have the latest versions.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Complete Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73fe7f07"
      },
      "source": [
        "### 6.1 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01aabac5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8960a11"
      },
      "source": [
        "### 6.2 Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3aca244"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Embedding dimension\n",
        "            max_len: Maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create matrix of [max_len, d_model] representing positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # Compute the div_term for sine and cosine functions\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (not a parameter, but should be saved in state_dict)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor of shape [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Add positional encoding to input\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189f1cd2"
      },
      "source": [
        "### 6.3 Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7f7ab1"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Dimension of model (embedding dimension)\n",
        "            num_heads: Number of attention heads\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q: Queries [batch_size, num_heads, seq_len, d_k]\n",
        "            K: Keys [batch_size, num_heads, seq_len, d_k]\n",
        "            V: Values [batch_size, num_heads, seq_len, d_k]\n",
        "            mask: Optional mask [batch_size, 1, 1, seq_len] or [batch_size, 1, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, d_k)\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "        Returns:\n",
        "            [batch_size, num_heads, seq_len, d_k]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"\n",
        "        Combine the heads back\n",
        "        Args:\n",
        "            x: [batch_size, num_heads, seq_len, d_k]\n",
        "        Returns:\n",
        "            [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, num_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            Q: Query [batch_size, seq_len, d_model]\n",
        "            K: Key [batch_size, seq_len, d_model]\n",
        "            V: Value [batch_size, seq_len, d_model]\n",
        "            mask: Optional mask\n",
        "        \"\"\"\n",
        "        # Linear projections\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # Apply attention\n",
        "        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads\n",
        "        output = self.combine_heads(attn_output)\n",
        "\n",
        "        # Final linear projection\n",
        "        output = self.W_o(output)\n",
        "\n",
        "        return output, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6dd8ff"
      },
      "source": [
        "### 6.4 Position-wise Feed-Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "333c2202"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            d_ff: Hidden dimension of feed-forward network (usually 4 * d_model)\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # First linear transformation with ReLU activation\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second linear transformation\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84c31e5"
      },
      "source": [
        "### 6.5 Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac804c00"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Single encoder layer with self-attention and feed-forward network\n",
        "\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            num_heads: Number of attention heads\n",
        "            d_ff: Feed-forward hidden dimension\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input [batch_size, seq_len, d_model]\n",
        "            mask: Optional attention mask\n",
        "        \"\"\"\n",
        "        # Self-attention with residual connection and layer norm\n",
        "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward with residual connection and layer norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe8652c2"
      },
      "source": [
        "### 6.6 Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba5a7350"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Single decoder layer with masked self-attention, cross-attention, and feed-forward\n",
        "\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            num_heads: Number of attention heads\n",
        "            d_ff: Feed-forward hidden dimension\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Decoder input [batch_size, tgt_seq_len, d_model]\n",
        "            encoder_output: Encoder output [batch_size, src_seq_len, d_model]\n",
        "            src_mask: Source mask for encoder output\n",
        "            tgt_mask: Target mask (causal mask) for decoder\n",
        "        \"\"\"\n",
        "        # Masked self-attention\n",
        "        self_attn_output, self_attn_weights = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "\n",
        "        # Cross-attention to encoder output\n",
        "        cross_attn_output, cross_attn_weights = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout3(ff_output))\n",
        "\n",
        "        return x, self_attn_weights, cross_attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f99be75"
      },
      "source": [
        "### 6.7 Complete Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45258548"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,\n",
        "                 max_seq_len=5000, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Complete Transformer model\n",
        "\n",
        "        Args:\n",
        "            src_vocab_size: Source vocabulary size\n",
        "            tgt_vocab_size: Target vocabulary size\n",
        "            d_model: Model dimension\n",
        "            num_heads: Number of attention heads\n",
        "            num_encoder_layers: Number of encoder layers\n",
        "            num_decoder_layers: Number of decoder layers\n",
        "            d_ff: Feed-forward hidden dimension\n",
        "            max_seq_len: Maximum sequence length\n",
        "            dropout: Dropout probability\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights using Xavier uniform initialization\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"\n",
        "        Generate causal mask for decoder (prevents attending to future positions)\n",
        "\n",
        "        Args:\n",
        "            sz: Sequence length\n",
        "        Returns:\n",
        "            Mask of shape [sz, sz]\n",
        "        \"\"\"\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        \"\"\"\n",
        "        Create mask for source sequence (mask padding tokens)\n",
        "\n",
        "        Args:\n",
        "            src: Source tensor [batch_size, src_seq_len]\n",
        "        Returns:\n",
        "            Mask [batch_size, 1, 1, src_seq_len]\n",
        "        \"\"\"\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        \"\"\"\n",
        "        Create mask for target sequence (causal mask + padding mask)\n",
        "\n",
        "        Args:\n",
        "            tgt: Target tensor [batch_size, tgt_seq_len]\n",
        "        Returns:\n",
        "            Mask [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "\n",
        "        # Padding mask\n",
        "        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Causal mask\n",
        "        tgt_sub_mask = self.generate_square_subsequent_mask(tgt_len).to(tgt.device)\n",
        "        tgt_sub_mask = tgt_sub_mask.unsqueeze(0)\n",
        "\n",
        "        # Combine masks\n",
        "        tgt_mask = tgt_pad_mask & (tgt_sub_mask == 0)\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        Encode source sequence\n",
        "\n",
        "        Args:\n",
        "            src: Source tensor [batch_size, src_seq_len]\n",
        "            src_mask: Source mask\n",
        "        Returns:\n",
        "            Encoder output [batch_size, src_seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Embedding + positional encoding\n",
        "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        encoder_self_attention_weights = []\n",
        "\n",
        "        # Pass through encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x, attn_weights = layer(x, src_mask)\n",
        "            encoder_self_attention_weights.append(attn_weights)\n",
        "\n",
        "        return x, encoder_self_attention_weights\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Decode target sequence\n",
        "\n",
        "        Args:\n",
        "            tgt: Target tensor [batch_size, tgt_seq_len]\n",
        "            encoder_output: Output from encoder\n",
        "            src_mask: Source mask\n",
        "            tgt_mask: Target mask\n",
        "        Returns:\n",
        "            Decoder output [batch_size, tgt_seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # Embedding + positional encoding\n",
        "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        decoder_self_attention_weights = []\n",
        "        decoder_cross_attention_weights = []\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.decoder_layers:\n",
        "            x, self_attn_weights, cross_attn_weights = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "            decoder_self_attention_weights.append(self_attn_weights)\n",
        "            decoder_cross_attention_weights.append(cross_attn_weights)\n",
        "\n",
        "        return x, decoder_self_attention_weights, decoder_cross_attention_weights\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Forward pass through the transformer\n",
        "\n",
        "        Args:\n",
        "            src: Source tensor [batch_size, src_seq_len]\n",
        "            tgt: Target tensor [batch_size, tgt_seq_len]\n",
        "        Returns:\n",
        "            Output logits [batch_size, tgt_seq_len, tgt_vocab_size]\n",
        "        \"\"\"\n",
        "        # Create masks\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        # Encode\n",
        "        encoder_output, _ = self.encode(src, src_mask)\n",
        "\n",
        "        # Decode\n",
        "        decoder_output, _, _ = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.fc_out(decoder_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "972ff5fc"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Training and Evaluation\n",
        "\n",
        "### 7.1 Sample Dataset (Translation Task)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bc31753"
      },
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    \"\"\"Simple dataset for sequence-to-sequence translation\"\"\"\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=50):\n",
        "        self.src_sentences = src_sentences\n",
        "        self.tgt_sentences = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.encode_sentence(self.src_sentences[idx], self.src_vocab)\n",
        "        tgt = self.encode_sentence(self.tgt_sentences[idx], self.tgt_vocab)\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "    def encode_sentence(self, sentence, vocab):\n",
        "        \"\"\"Convert sentence to token indices\"\"\"\n",
        "        tokens = sentence.lower().split()\n",
        "        indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "        indices = [vocab['<sos>']] + indices + [vocab['<eos>']]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indices) < self.max_len:\n",
        "            indices += [vocab['<pad>']] * (self.max_len - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_len]\n",
        "\n",
        "        return indices\n",
        "\n",
        "# Create simple vocabularies\n",
        "def create_vocab(sentences):\n",
        "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "    idx = 4\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb299885"
      },
      "source": [
        "### 7.2 Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b3a37a0"
      },
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Training loop for the transformer model\n",
        "\n",
        "    Args:\n",
        "        model: Transformer model\n",
        "        train_loader: DataLoader for training data\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        device: Device to train on (cuda/cpu)\n",
        "        num_epochs: Number of training epochs\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Target input and output for teacher forcing\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            # The forward method of Transformer now returns attention weights, but train_model only needs the output.\n",
        "            # The Transformer.forward method already ignores the attention weights it gets from encode/decode, so no change needed here.\n",
        "            output = model(src, tgt_input)\n",
        "\n",
        "            # Calculate loss\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt_output)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09312001"
      },
      "source": [
        "### 7.3 Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c4db59c"
      },
      "source": [
        "def translate(model, src_sentence, src_vocab, tgt_vocab, device, max_len=50):\n",
        "    \"\"\"\n",
        "    Translate a source sentence using the trained model\n",
        "\n",
        "    Args:\n",
        "        model: Trained transformer model\n",
        "        src_sentence: Source sentence string\n",
        "        src_vocab: Source vocabulary\n",
        "        tgt_vocab: Target vocabulary\n",
        "        device: Device\n",
        "        max_len: Maximum generation length\n",
        "    Returns:\n",
        "        Translated sentence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode source sentence\n",
        "    src_tokens = src_sentence.lower().split()\n",
        "    src_indices = [src_vocab['<sos>']] + [src_vocab.get(t, src_vocab['<unk>']) for t in src_tokens] + [src_vocab['<eos>']]\n",
        "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
        "\n",
        "    # Create reverse vocabulary for target\n",
        "    idx_to_word = {idx: word for word, idx in tgt_vocab.items()}\n",
        "\n",
        "    # Start with <sos> token\n",
        "    tgt_indices = [tgt_vocab['<sos>']]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encode source\n",
        "        encoder_output, _ = model.encode(src_tensor, src_mask=model.make_src_mask(src_tensor)) # Modified to unpack attention weights\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
        "            tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
        "\n",
        "            # Decode\n",
        "            decoder_output, _, _ = model.decode(tgt_tensor, encoder_output, src_mask=model.make_src_mask(src_tensor), tgt_mask=tgt_mask) # Modified to unpack attention weights\n",
        "            output = model.fc_out(decoder_output)\n",
        "\n",
        "            # Get next token\n",
        "            next_token = output.argmax(dim=-1)[:, -1].item()\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "            # Stop if we generate <eos>\n",
        "            if next_token == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "    # Convert indices to words\n",
        "    translated = [idx_to_word.get(idx, '<unk>') for idx in tgt_indices[1:-1]]\n",
        "\n",
        "    return ' '.join(translated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f0df35e"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Complete Google Colab Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bf3c31",
        "outputId": "f0ef84de-80f3-48f4-91cd-cb007bc551a9"
      },
      "source": [
        "# ==========================\n",
        "# COMPLETE COLAB IMPLEMENTATION\n",
        "# ==========================\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Sample data (English to French translation - toy example)\n",
        "src_sentences = [\n",
        "    'hello world',\n",
        "    'how are you',\n",
        "    'good morning',\n",
        "    'thank you very much',\n",
        "    'see you later',\n",
        "]\n",
        "\n",
        "tgt_sentences = [\n",
        "    'bonjour monde',\n",
        "    'comment allez vous',\n",
        "    'bonjour',\n",
        "    'merci beaucoup',\n",
        "    'a bientot',\n",
        "]\n",
        "\n",
        "# Create vocabularies\n",
        "src_vocab = create_vocab(src_sentences)\n",
        "tgt_vocab = create_vocab(tgt_sentences)\n",
        "\n",
        "print(f'Source vocab size: {len(src_vocab)}')\n",
        "print(f'Target vocab size: {len(tgt_vocab)}')\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = TranslationDataset(src_sentences, tgt_sentences, src_vocab, tgt_vocab)\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(src_vocab),\n",
        "    tgt_vocab_size=len(tgt_vocab),\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_encoder_layers=2,\n",
        "    num_decoder_layers=2,\n",
        "    d_ff=512,\n",
        "    max_seq_len=50,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Total parameters: {total_params:,}')\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# Train the model\n",
        "print('\\nStarting training...\\n')\n",
        "train_model(model, train_loader, criterion, optimizer, device, num_epochs=100)\n",
        "\n",
        "# Test translation\n",
        "print('\\n' + '='*50)\n",
        "print('Testing translations:')\n",
        "print('='*50)\n",
        "\n",
        "test_sentences = ['hello world', 'thank you very much']\n",
        "for sentence in test_sentences:\n",
        "    translation = translate(model, sentence, src_vocab, tgt_vocab, device)\n",
        "    print(f'Input: {sentence}')\n",
        "    print(f'Translation: {translation}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Source vocab size: 16\n",
            "Target vocab size: 13\n",
            "Total parameters: 931,085\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch: 1/100, Batch: 0, Loss: 3.0073\n",
            "Epoch 1 completed. Average Loss: 2.7655\n",
            "\n",
            "Epoch: 2/100, Batch: 0, Loss: 2.7612\n",
            "Epoch 2 completed. Average Loss: 2.4522\n",
            "\n",
            "Epoch: 3/100, Batch: 0, Loss: 2.4198\n",
            "Epoch 3 completed. Average Loss: 2.1736\n",
            "\n",
            "Epoch: 4/100, Batch: 0, Loss: 2.5419\n",
            "Epoch 4 completed. Average Loss: 2.0651\n",
            "\n",
            "Epoch: 5/100, Batch: 0, Loss: 1.6011\n",
            "Epoch 5 completed. Average Loss: 1.9731\n",
            "\n",
            "Epoch: 6/100, Batch: 0, Loss: 1.6142\n",
            "Epoch 6 completed. Average Loss: 1.8440\n",
            "\n",
            "Epoch: 7/100, Batch: 0, Loss: 2.1556\n",
            "Epoch 7 completed. Average Loss: 1.9361\n",
            "\n",
            "Epoch: 8/100, Batch: 0, Loss: 1.1978\n",
            "Epoch 8 completed. Average Loss: 1.5961\n",
            "\n",
            "Epoch: 9/100, Batch: 0, Loss: 1.5651\n",
            "Epoch 9 completed. Average Loss: 1.2369\n",
            "\n",
            "Epoch: 10/100, Batch: 0, Loss: 1.5831\n",
            "Epoch 10 completed. Average Loss: 1.6639\n",
            "\n",
            "Epoch: 11/100, Batch: 0, Loss: 1.8198\n",
            "Epoch 11 completed. Average Loss: 1.5448\n",
            "\n",
            "Epoch: 12/100, Batch: 0, Loss: 1.0863\n",
            "Epoch 12 completed. Average Loss: 1.1664\n",
            "\n",
            "Epoch: 13/100, Batch: 0, Loss: 0.7662\n",
            "Epoch 13 completed. Average Loss: 1.1495\n",
            "\n",
            "Epoch: 14/100, Batch: 0, Loss: 1.1561\n",
            "Epoch 14 completed. Average Loss: 0.9393\n",
            "\n",
            "Epoch: 15/100, Batch: 0, Loss: 1.0254\n",
            "Epoch 15 completed. Average Loss: 1.2670\n",
            "\n",
            "Epoch: 16/100, Batch: 0, Loss: 0.8504\n",
            "Epoch 16 completed. Average Loss: 0.7743\n",
            "\n",
            "Epoch: 17/100, Batch: 0, Loss: 0.8642\n",
            "Epoch 17 completed. Average Loss: 0.7116\n",
            "\n",
            "Epoch: 18/100, Batch: 0, Loss: 0.6370\n",
            "Epoch 18 completed. Average Loss: 0.7605\n",
            "\n",
            "Epoch: 19/100, Batch: 0, Loss: 0.5435\n",
            "Epoch 19 completed. Average Loss: 0.8111\n",
            "\n",
            "Epoch: 20/100, Batch: 0, Loss: 0.3808\n",
            "Epoch 20 completed. Average Loss: 0.6711\n",
            "\n",
            "Epoch: 21/100, Batch: 0, Loss: 0.4926\n",
            "Epoch 21 completed. Average Loss: 0.5672\n",
            "\n",
            "Epoch: 22/100, Batch: 0, Loss: 0.8163\n",
            "Epoch 22 completed. Average Loss: 0.5946\n",
            "\n",
            "Epoch: 23/100, Batch: 0, Loss: 0.5605\n",
            "Epoch 23 completed. Average Loss: 0.5697\n",
            "\n",
            "Epoch: 24/100, Batch: 0, Loss: 0.2825\n",
            "Epoch 24 completed. Average Loss: 0.4151\n",
            "\n",
            "Epoch: 25/100, Batch: 0, Loss: 0.7752\n",
            "Epoch 25 completed. Average Loss: 0.4883\n",
            "\n",
            "Epoch: 26/100, Batch: 0, Loss: 0.2785\n",
            "Epoch 26 completed. Average Loss: 0.3804\n",
            "\n",
            "Epoch: 27/100, Batch: 0, Loss: 0.4347\n",
            "Epoch 27 completed. Average Loss: 0.4846\n",
            "\n",
            "Epoch: 28/100, Batch: 0, Loss: 0.3272\n",
            "Epoch 28 completed. Average Loss: 0.3645\n",
            "\n",
            "Epoch: 29/100, Batch: 0, Loss: 0.3571\n",
            "Epoch 29 completed. Average Loss: 0.2211\n",
            "\n",
            "Epoch: 30/100, Batch: 0, Loss: 0.3472\n",
            "Epoch 30 completed. Average Loss: 0.2600\n",
            "\n",
            "Epoch: 31/100, Batch: 0, Loss: 0.3121\n",
            "Epoch 31 completed. Average Loss: 0.2256\n",
            "\n",
            "Epoch: 32/100, Batch: 0, Loss: 0.3204\n",
            "Epoch 32 completed. Average Loss: 0.2163\n",
            "\n",
            "Epoch: 33/100, Batch: 0, Loss: 0.3380\n",
            "Epoch 33 completed. Average Loss: 0.2345\n",
            "\n",
            "Epoch: 34/100, Batch: 0, Loss: 0.3529\n",
            "Epoch 34 completed. Average Loss: 0.2252\n",
            "\n",
            "Epoch: 35/100, Batch: 0, Loss: 0.1778\n",
            "Epoch 35 completed. Average Loss: 0.1513\n",
            "\n",
            "Epoch: 36/100, Batch: 0, Loss: 0.2450\n",
            "Epoch 36 completed. Average Loss: 0.1485\n",
            "\n",
            "Epoch: 37/100, Batch: 0, Loss: 0.0879\n",
            "Epoch 37 completed. Average Loss: 0.1120\n",
            "\n",
            "Epoch: 38/100, Batch: 0, Loss: 0.1171\n",
            "Epoch 38 completed. Average Loss: 0.1126\n",
            "\n",
            "Epoch: 39/100, Batch: 0, Loss: 0.2468\n",
            "Epoch 39 completed. Average Loss: 0.2169\n",
            "\n",
            "Epoch: 40/100, Batch: 0, Loss: 0.1844\n",
            "Epoch 40 completed. Average Loss: 0.1672\n",
            "\n",
            "Epoch: 41/100, Batch: 0, Loss: 0.0424\n",
            "Epoch 41 completed. Average Loss: 0.2057\n",
            "\n",
            "Epoch: 42/100, Batch: 0, Loss: 0.1697\n",
            "Epoch 42 completed. Average Loss: 0.1776\n",
            "\n",
            "Epoch: 43/100, Batch: 0, Loss: 0.1387\n",
            "Epoch 43 completed. Average Loss: 0.2371\n",
            "\n",
            "Epoch: 44/100, Batch: 0, Loss: 0.3477\n",
            "Epoch 44 completed. Average Loss: 0.1767\n",
            "\n",
            "Epoch: 45/100, Batch: 0, Loss: 0.0467\n",
            "Epoch 45 completed. Average Loss: 0.1816\n",
            "\n",
            "Epoch: 46/100, Batch: 0, Loss: 0.0845\n",
            "Epoch 46 completed. Average Loss: 0.1745\n",
            "\n",
            "Epoch: 47/100, Batch: 0, Loss: 0.0911\n",
            "Epoch 47 completed. Average Loss: 0.0537\n",
            "\n",
            "Epoch: 48/100, Batch: 0, Loss: 0.0674\n",
            "Epoch 48 completed. Average Loss: 0.0892\n",
            "\n",
            "Epoch: 49/100, Batch: 0, Loss: 0.0821\n",
            "Epoch 49 completed. Average Loss: 0.0794\n",
            "\n",
            "Epoch: 50/100, Batch: 0, Loss: 0.0825\n",
            "Epoch 50 completed. Average Loss: 0.0740\n",
            "\n",
            "Epoch: 51/100, Batch: 0, Loss: 0.0763\n",
            "Epoch 51 completed. Average Loss: 0.0657\n",
            "\n",
            "Epoch: 52/100, Batch: 0, Loss: 0.0330\n",
            "Epoch 52 completed. Average Loss: 0.0252\n",
            "\n",
            "Epoch: 53/100, Batch: 0, Loss: 0.0306\n",
            "Epoch 53 completed. Average Loss: 0.0498\n",
            "\n",
            "Epoch: 54/100, Batch: 0, Loss: 0.0632\n",
            "Epoch 54 completed. Average Loss: 0.0345\n",
            "\n",
            "Epoch: 55/100, Batch: 0, Loss: 0.0742\n",
            "Epoch 55 completed. Average Loss: 0.0484\n",
            "\n",
            "Epoch: 56/100, Batch: 0, Loss: 0.0359\n",
            "Epoch 56 completed. Average Loss: 0.0290\n",
            "\n",
            "Epoch: 57/100, Batch: 0, Loss: 0.0275\n",
            "Epoch 57 completed. Average Loss: 0.0703\n",
            "\n",
            "Epoch: 58/100, Batch: 0, Loss: 0.0435\n",
            "Epoch 58 completed. Average Loss: 0.0231\n",
            "\n",
            "Epoch: 59/100, Batch: 0, Loss: 0.1289\n",
            "Epoch 59 completed. Average Loss: 0.1083\n",
            "\n",
            "Epoch: 60/100, Batch: 0, Loss: 0.0258\n",
            "Epoch 60 completed. Average Loss: 0.0371\n",
            "\n",
            "Epoch: 61/100, Batch: 0, Loss: 0.0258\n",
            "Epoch 61 completed. Average Loss: 0.0250\n",
            "\n",
            "Epoch: 62/100, Batch: 0, Loss: 0.0131\n",
            "Epoch 62 completed. Average Loss: 0.0163\n",
            "\n",
            "Epoch: 63/100, Batch: 0, Loss: 0.0147\n",
            "Epoch 63 completed. Average Loss: 0.0224\n",
            "\n",
            "Epoch: 64/100, Batch: 0, Loss: 0.0188\n",
            "Epoch 64 completed. Average Loss: 0.0213\n",
            "\n",
            "Epoch: 65/100, Batch: 0, Loss: 0.0212\n",
            "Epoch 65 completed. Average Loss: 0.0121\n",
            "\n",
            "Epoch: 66/100, Batch: 0, Loss: 0.0198\n",
            "Epoch 66 completed. Average Loss: 0.0294\n",
            "\n",
            "Epoch: 67/100, Batch: 0, Loss: 0.0082\n",
            "Epoch 67 completed. Average Loss: 0.0182\n",
            "\n",
            "Epoch: 68/100, Batch: 0, Loss: 0.0175\n",
            "Epoch 68 completed. Average Loss: 0.0154\n",
            "\n",
            "Epoch: 69/100, Batch: 0, Loss: 0.0052\n",
            "Epoch 69 completed. Average Loss: 0.0400\n",
            "\n",
            "Epoch: 70/100, Batch: 0, Loss: 0.0327\n",
            "Epoch 70 completed. Average Loss: 0.0166\n",
            "\n",
            "Epoch: 71/100, Batch: 0, Loss: 0.0112\n",
            "Epoch 71 completed. Average Loss: 0.0376\n",
            "\n",
            "Epoch: 72/100, Batch: 0, Loss: 0.0074\n",
            "Epoch 72 completed. Average Loss: 0.0071\n",
            "\n",
            "Epoch: 73/100, Batch: 0, Loss: 0.0112\n",
            "Epoch 73 completed. Average Loss: 0.0080\n",
            "\n",
            "Epoch: 74/100, Batch: 0, Loss: 0.0101\n",
            "Epoch 74 completed. Average Loss: 0.0082\n",
            "\n",
            "Epoch: 75/100, Batch: 0, Loss: 0.0166\n",
            "Epoch 75 completed. Average Loss: 0.0104\n",
            "\n",
            "Epoch: 76/100, Batch: 0, Loss: 0.0070\n",
            "Epoch 76 completed. Average Loss: 0.0313\n",
            "\n",
            "Epoch: 77/100, Batch: 0, Loss: 0.0219\n",
            "Epoch 77 completed. Average Loss: 0.0150\n",
            "\n",
            "Epoch: 78/100, Batch: 0, Loss: 0.0076\n",
            "Epoch 78 completed. Average Loss: 0.0083\n",
            "\n",
            "Epoch: 79/100, Batch: 0, Loss: 0.0083\n",
            "Epoch 79 completed. Average Loss: 0.0055\n",
            "\n",
            "Epoch: 80/100, Batch: 0, Loss: 0.0078\n",
            "Epoch 80 completed. Average Loss: 0.0185\n",
            "\n",
            "Epoch: 81/100, Batch: 0, Loss: 0.0222\n",
            "Epoch 81 completed. Average Loss: 0.0184\n",
            "\n",
            "Epoch: 82/100, Batch: 0, Loss: 0.0158\n",
            "Epoch 82 completed. Average Loss: 0.0092\n",
            "\n",
            "Epoch: 83/100, Batch: 0, Loss: 0.0076\n",
            "Epoch 83 completed. Average Loss: 0.0065\n",
            "\n",
            "Epoch: 84/100, Batch: 0, Loss: 0.0058\n",
            "Epoch 84 completed. Average Loss: 0.0042\n",
            "\n",
            "Epoch: 85/100, Batch: 0, Loss: 0.0029\n",
            "Epoch 85 completed. Average Loss: 0.0050\n",
            "\n",
            "Epoch: 86/100, Batch: 0, Loss: 0.0038\n",
            "Epoch 86 completed. Average Loss: 0.0070\n",
            "\n",
            "Epoch: 87/100, Batch: 0, Loss: 0.0081\n",
            "Epoch 87 completed. Average Loss: 0.0093\n",
            "\n",
            "Epoch: 88/100, Batch: 0, Loss: 0.0112\n",
            "Epoch 88 completed. Average Loss: 0.0197\n",
            "\n",
            "Epoch: 89/100, Batch: 0, Loss: 0.0057\n",
            "Epoch 89 completed. Average Loss: 0.0055\n",
            "\n",
            "Epoch: 90/100, Batch: 0, Loss: 0.0014\n",
            "Epoch 90 completed. Average Loss: 0.0024\n",
            "\n",
            "Epoch: 91/100, Batch: 0, Loss: 0.0073\n",
            "Epoch 91 completed. Average Loss: 0.0068\n",
            "\n",
            "Epoch: 92/100, Batch: 0, Loss: 0.0055\n",
            "Epoch 92 completed. Average Loss: 0.0168\n",
            "\n",
            "Epoch: 93/100, Batch: 0, Loss: 0.0083\n",
            "Epoch 93 completed. Average Loss: 0.0069\n",
            "\n",
            "Epoch: 94/100, Batch: 0, Loss: 0.0021\n",
            "Epoch 94 completed. Average Loss: 0.0040\n",
            "\n",
            "Epoch: 95/100, Batch: 0, Loss: 0.0033\n",
            "Epoch 95 completed. Average Loss: 0.0041\n",
            "\n",
            "Epoch: 96/100, Batch: 0, Loss: 0.0055\n",
            "Epoch 96 completed. Average Loss: 0.0055\n",
            "\n",
            "Epoch: 97/100, Batch: 0, Loss: 0.0051\n",
            "Epoch 97 completed. Average Loss: 0.0031\n",
            "\n",
            "Epoch: 98/100, Batch: 0, Loss: 0.0029\n",
            "Epoch 98 completed. Average Loss: 0.0029\n",
            "\n",
            "Epoch: 99/100, Batch: 0, Loss: 0.0037\n",
            "Epoch 99 completed. Average Loss: 0.0027\n",
            "\n",
            "Epoch: 100/100, Batch: 0, Loss: 0.0034\n",
            "Epoch 100 completed. Average Loss: 0.0021\n",
            "\n",
            "\n",
            "==================================================\n",
            "Testing translations:\n",
            "==================================================\n",
            "Input: hello world\n",
            "Translation: bonjour monde\n",
            "\n",
            "Input: thank you very much\n",
            "Translation: merci beaucoup\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5321ebf3"
      },
      "source": [
        "---\n",
        "\n",
        "## 9. Optimization Techniques\n",
        "\n",
        "### 9.1 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d066b05"
      },
      "source": [
        "class WarmupScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        lr = self.d_model ** (-0.5) * min(self.step_num ** (-0.5),\n",
        "                                          self.step_num * self.warmup_steps ** (-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "596b9049"
      },
      "source": [
        "### 9.2 Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8350e7f9"
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1, vocab_size=None, padding_idx=0):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.vocab_size = vocab_size\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: Predictions [batch_size * seq_len, vocab_size]\n",
        "            target: Ground truth labels [batch_size * seq_len]\n",
        "        \"\"\"\n",
        "        vocab_size = pred.size(-1)\n",
        "\n",
        "        # Create smoothed target distribution\n",
        "        smooth_target = torch.zeros_like(pred)\n",
        "        smooth_target.fill_(self.smoothing / (vocab_size - 2))  # -2 for true class and padding\n",
        "        smooth_target.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        smooth_target[:, self.padding_idx] = 0\n",
        "\n",
        "        # Mask padding positions\n",
        "        mask = (target != self.padding_idx).unsqueeze(1)\n",
        "        smooth_target = smooth_target * mask\n",
        "\n",
        "        # Calculate loss\n",
        "        log_probs = F.log_softmax(pred, dim=-1)\n",
        "        loss = -(smooth_target * log_probs).sum(dim=-1)\n",
        "        loss = loss[mask.squeeze()].mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66a7376"
      },
      "source": [
        "### 9.3 Gradient Accumulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74b81a99"
      },
      "source": [
        "def train_with_accumulation(model, train_loader, criterion, optimizer, device,\n",
        "                           accumulation_steps=4, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Training with gradient accumulation\n",
        "\n",
        "    Args:\n",
        "        accumulation_steps: Number of steps to accumulate gradients\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt_input)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            # Calculate loss and normalize by accumulation steps\n",
        "            loss = criterion(output, tgt_output) / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights every accumulation_steps\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed. Average Loss: {total_loss/len(train_loader):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4975e74"
      },
      "source": [
        "### 9.4 Mixed Precision Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cd972a2"
      },
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "def train_mixed_precision(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Training with automatic mixed precision (FP16)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (src, tgt) in enumerate(train_loader):\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with autocast\n",
        "            with autocast():\n",
        "                output = model(src, tgt_input)\n",
        "                output = output.reshape(-1, output.shape[-1])\n",
        "                tgt_output = tgt_output.reshape(-1)\n",
        "                loss = criterion(output, tgt_output)\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed. Average Loss: {total_loss/len(train_loader):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae66f5dd"
      },
      "source": [
        "### 9.5 Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e191b0e"
      },
      "source": [
        "def visualize_attention(model, src_sentence, tgt_sentence, src_vocab, tgt_vocab,\n",
        "                       device, layer_idx=0, head_idx=0):\n",
        "    \"\"\"\n",
        "    Visualize attention weights for a specific layer and head\n",
        "\n",
        "    Args:\n",
        "        layer_idx: Which encoder/decoder layer to visualize\n",
        "        head_idx: Which attention head to visualize\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode sentences\n",
        "    src_tokens = ['<sos>'] + src_sentence.lower().split() + ['<eos>']\n",
        "    tgt_tokens = ['<sos>'] + tgt_sentence.lower().split() + ['<eos>']\n",
        "\n",
        "    src_indices = torch.tensor([[src_vocab.get(t, src_vocab['<unk>']) for t in src_tokens]]).to(device)\n",
        "    tgt_indices = torch.tensor([[tgt_vocab.get(t, tgt_vocab['<unk>']) for t in tgt_tokens]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get attention weights from a specific layer\n",
        "        src_mask = model.make_src_mask(src_indices)\n",
        "        encoder_output = model.encode(src_indices, src_mask)\n",
        "\n",
        "        # Extract attention from specific layer and head\n",
        "        tgt_mask = model.make_tgt_mask(tgt_indices)\n",
        "\n",
        "        # For visualization, we'd need to modify the forward pass to return attention weights\n",
        "        # This is a simplified version\n",
        "\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # attention_weights would be extracted from the model\n",
        "    # plt.imshow(attention_weights, cmap='viridis')\n",
        "    plt.xlabel('Source Tokens')\n",
        "    plt.ylabel('Target Tokens')\n",
        "    plt.xticks(range(len(src_tokens)), src_tokens, rotation=45)\n",
        "    plt.yticks(range(len(tgt_tokens)), tgt_tokens)\n",
        "    plt.colorbar()\n",
        "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a525c858"
      },
      "source": [
        "---\n",
        "\n",
        "## 10. Advanced Techniques for Scaling to LLMs\n",
        "\n",
        "### 10.1 Decoder-Only Architecture (GPT-style)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59a733ee"
      },
      "source": [
        "class GPTTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder-only transformer (GPT-style) for language modeling\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12,\n",
        "                 d_ff=3072, max_seq_len=1024, dropout=0.1):\n",
        "        super(GPTTransformer, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        # Decoder layers only (no encoder)\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Create causal mask\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "        mask = mask.to(x.device)\n",
        "\n",
        "        # Embed and add positional encoding\n",
        "        x = self.token_embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.fc_out(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Single decoder block for GPT-style models\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Self-attention with causal mask\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask.unsqueeze(0).unsqueeze(0))\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout2(ff_output))\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ade663f"
      },
      "source": [
        "### 10.2 Rotary Position Embeddings (RoPE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c868520"
      },
      "source": [
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary Position Embedding (RoPE) as used in models like LLaMA\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
        "        super(RotaryPositionalEmbedding, self).__init__()\n",
        "\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch, seq_len, num_heads, head_dim]\n",
        "            seq_len: Sequence length\n",
        "        \"\"\"\n",
        "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        return emb.cos()[None, :, None, :], emb.sin()[None, :, None, :]\n",
        "\n",
        "def apply_rotary_emb(x, cos, sin):\n",
        "    \"\"\"Apply rotary embeddings to input tensor\"\"\"\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat([\n",
        "        x1 * cos - x2 * sin,\n",
        "        x1 * sin + x2 * cos\n",
        "    ], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed20779c"
      },
      "source": [
        "### 10.3 Key-Value Caching for Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a37d05f5"
      },
      "source": [
        "def generate_with_cache(model, prompt_ids, max_new_tokens=50, temperature=1.0, top_k=50):\n",
        "    \"\"\"\n",
        "    Generate text with KV caching for efficient inference\n",
        "\n",
        "    Args:\n",
        "        model: GPT-style transformer model\n",
        "        prompt_ids: Input token IDs [1, prompt_len]\n",
        "        max_new_tokens: Number of tokens to generate\n",
        "        temperature: Sampling temperature\n",
        "        top_k: Top-k sampling parameter\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = prompt_ids\n",
        "    past_key_values = None\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            # Forward pass (with caching in production models)\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "                logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "    return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaa754d1"
      },
      "source": [
        "---\n",
        "\n",
        "## 11. Best Practices and Tips\n",
        "\n",
        "### Memory Optimization\n",
        "1. **Gradient Checkpointing**: Trade compute for memory\n",
        "2. **Mixed Precision**: Use FP16 for faster training\n",
        "3. **Batch Size Tuning**: Find optimal batch size for your GPU\n",
        "4. **Gradient Accumulation**: Simulate larger batches\n",
        "\n",
        "### Training Stability\n",
        "1. **Layer Normalization**: Use pre-norm (before attention) for deeper models\n",
        "2. **Gradient Clipping**: Prevent exploding gradients\n",
        "3. **Warmup**: Gradually increase learning rate\n",
        "4. **Weight Initialization**: Use Xavier or He initialization\n",
        "\n",
        "### Hyperparameter Recommendations\n",
        "```python\n",
        "# Small model (testing)\n",
        "d_model = 256\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "d_ff = 1024\n",
        "\n",
        "# Medium model (GPT-2 small)\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "num_layers = 12\n",
        "d_ff = 3072\n",
        "\n",
        "# Large model (GPT-2 large)\n",
        "d_model = 1280\n",
        "num_heads = 20\n",
        "num_layers = 36\n",
        "d_ff = 5120\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Common Issues and Solutions\n",
        "\n",
        "### Issue 1: Out of Memory\n",
        "**Solutions:**\n",
        "- Reduce batch size\n",
        "- Use gradient accumulation\n",
        "- Enable mixed precision training\n",
        "- Use gradient checkpointing\n",
        "\n",
        "### Issue 2: Loss Not Decreasing\n",
        "**Solutions:**\n",
        "- Check learning rate (try 1e-4 to 1e-3)\n",
        "- Verify data preprocessing\n",
        "- Check for gradient clipping\n",
        "- Ensure masks are correct\n",
        "\n",
        "### Issue 3: Poor Generation Quality\n",
        "**Solutions:**\n",
        "- Train longer\n",
        "- Increase model size\n",
        "- Use better tokenization\n",
        "- Implement beam search instead of greedy decoding\n",
        "\n",
        "---\n",
        "\n",
        "## 13. Further Reading and Resources\n",
        "\n",
        "### Papers\n",
        "1. **\"Attention Is All You Need\"** (Vaswani et al., 2017) - Original transformer paper\n",
        "2. **\"BERT\"** (Devlin et al., 2018) - Bidirectional transformers\n",
        "3. **\"GPT-2\"** (Radford et al., 2019) - Language models are unsupervised multitask learners\n",
        "4. **\"GPT-3\"** (Brown et al., 2020) - Few-shot learning with large models\n",
        "\n",
        "### Online Resources\n",
        "- Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/\n",
        "- Hugging Face Transformers: https://huggingface.co/docs/transformers/\n",
        "- PyTorch Tutorials: https://pytorch.org/tutorials/\n",
        "\n",
        "### Next Steps\n",
        "1. Implement attention visualization\n",
        "2. Add beam search decoding\n",
        "3. Experiment with different positional encodings\n",
        "4. Train on larger datasets (WMT, WikiText)\n",
        "5. Implement model parallelism for very large models\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This guide provides a complete foundation for understanding and implementing transformers. The architecture you've built here is the same fundamental design powering modern LLMs, just at different scales. To scale to production LLMs:\n",
        "\n",
        "1. **Scale up**: More layers, larger dimensions, more data\n",
        "2. **Optimize**: Use efficient attention variants (FlashAttention)\n",
        "3. **Distribute**: Model and data parallelism across GPUs\n",
        "4. **Fine-tune**: Instruction tuning, RLHF for alignment\n",
        "\n",
        "Happy building! "
      ]
    }
  ]
}